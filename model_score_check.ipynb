{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종 모델 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = (\n",
    "\"\"\"\n",
    "절대 한국어로만 답변하십시오. 영어를 사용하지 마십시오.  \n",
    "You must always respond in Korean. Do not use English.  \n",
    "\n",
    "당신은 대한민국 현행 법령을 기반으로 답변하는 RAG 챗봇입니다.  \n",
    "아래 지침을 따르고, 명확하고 간결한 답변을 제공하세요.\n",
    "\n",
    "- 질문이 단순하면 한 문장으로 답변하세요.  \n",
    "- 질문이 복잡하면 2~3문장 이내로 답변하세요.  \n",
    "- 법 조항을 직접 나열하지 말고, 질문과 관련된 핵심 내용을 요약하여 자연스럽게 설명하세요.  \n",
    "- 각 문장에 해당하는 법적 근거 조항을 괄호 안에 정확히 명시하세요. (예: 도로교통법 제12조 제1항)  \n",
    "- 질문을 반복하지 말고, 불필요한 형식적인 표현을 포함하지 마세요.  \n",
    "- **정확도, 확률, 신뢰도 같은 수치를 답변에 절대 포함하지 마세요.**  \n",
    "- Context 내에서만 답변을 생성하고, 모르는 내용은 `\"제가 제공할 수 있는 정보가 없습니다.\"`라고만 답변하세요.  \n",
    "\n",
    "{question}에 대한 답변을 다음 Context를 참고하여 작성하세요.\n",
    "\n",
    "Context:  \n",
    "{context}  \n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "SYS_PROMPT_TEMPLATE = (\n",
    "\"\"\"\n",
    "당신은 대한민국 현행 법령을 기반으로 작동하는 RAG 챗봇입니다.  \n",
    "절대 한국어로만 답변하십시오. 영어를 사용하지 마십시오.  \n",
    "\n",
    "답변을 생성할 때, 다음 원칙을 따르세요.\n",
    "\n",
    "1. **간결한 답변 제공**  \n",
    "   - 단순한 질문 → 한 문장  \n",
    "   - 복잡한 질문 → 2~3문장  \n",
    "   - 불필요한 설명, 배경 정보, 반복 표현 금지  \n",
    "\n",
    "2. **법 조항 요약 방식**  \n",
    "   - 법 조항을 직접 나열하지 않고 질문과 관련된 핵심 내용을 요약  \n",
    "   - 각 문장에 해당하는 법적 근거 조항을 괄호 안에 명확히 표기 (예: 도로교통법 제17조 제2항)\n",
    "\n",
    "3. **불필요한 정보 제한**  \n",
    "   - \"정확도\", \"신뢰도\", \"확률\" 등의 수치는 절대 포함 금지  \n",
    "   - \"또한\", \"추가로\", \"이 외에도\"와 같은 확장 표현 사용 금지  \n",
    "\n",
    "4. **추론 및 임의 해석 금지**  \n",
    "   - 제공된 Context 범위 내에서만 답변 작성  \n",
    "   - 필요한 정보가 없으면 `\"제가 제공할 수 있는 정보가 없습니다.\"`라고만 답변  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "class SentenceTransformerEmbedding:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True, device=DEVICE)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, convert_to_tensor=True).cpu().tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text], convert_to_tensor=True).cpu().tolist()[0]\n",
    "\n",
    "\n",
    "class ReRanker:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.reranker = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True).to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    def re_rank(self, query, docs):\n",
    "        if not docs:\n",
    "            return []\n",
    "\n",
    "        inputs = self.tokenizer([(query, doc) for doc in docs], padding=True, truncation=True, return_tensors='pt').to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.reranker(**inputs)\n",
    "            scores = outputs.logits.squeeze(-1).cpu()\n",
    "        \n",
    "        return [doc for _, doc in sorted(zip(scores, docs), reverse=True)]\n",
    "\n",
    "\n",
    "def load_chroma_retriever_with_embedding_and_reranker(chroma_dir, embedding_model_name=\"jinaai/jina-embeddings-v3\", reranker_model_name=\"BAAI/bge-reranker-base\", k=10):\n",
    "    embedding_model = SentenceTransformerEmbedding(embedding_model_name)\n",
    "    vectorstore = Chroma(persist_directory=chroma_dir, embedding_function=embedding_model)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': k})\n",
    "    reranker = ReRanker(reranker_model_name)\n",
    "    return retriever, reranker\n",
    "\n",
    "\n",
    "def setup_rag_system_with_reranker(llm_base_url):\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=llm_base_url,\n",
    "        api_key=\"lm-studio\",\n",
    "        model=\"teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYS_PROMPT_TEMPLATE),\n",
    "        (\"user\", RAG_PROMPT_TEMPLATE),\n",
    "    ])\n",
    "    rag_chain = prompt_template | llm | StrOutputParser()\n",
    "    return llm, rag_chain\n",
    "\n",
    "\n",
    "def run_pipeline(question, llm_base_url=\"http://localhost:1234/v1\", chroma_dir=\"chroma_data_folder_final_copy\", reranker_model_name=\"BAAI/bge-reranker-base\"):\n",
    "    if not question.strip():\n",
    "        return \"질문을 입력해주세요.\", [], []\n",
    "\n",
    "    retriever, reranker = load_chroma_retriever_with_embedding_and_reranker(\n",
    "        chroma_dir=chroma_dir, reranker_model_name=reranker_model_name\n",
    "    )\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    if not retrieved_docs:\n",
    "        return \"제가 제공할 수 있는 정보가 없습니다.\", [], []\n",
    "\n",
    "    ranked_docs = reranker.re_rank(question, [doc.page_content for doc in retrieved_docs])\n",
    "    if not ranked_docs:\n",
    "        return \"제가 제공할 수 있는 정보가 없습니다.\", retrieved_docs, []\n",
    "\n",
    "    context = \" \".join(ranked_docs)\n",
    "    llm, rag_chain = setup_rag_system_with_reranker(llm_base_url)\n",
    "    response = rag_chain.invoke({\"question\": question, \"context\": context})\n",
    "    return response, retrieved_docs, ranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "simple_question1 = pd.read_csv('score_check/question/simple_questions_1.csv')\n",
    "simple_question2 = pd.read_csv('score_check/question/simple_questions_2.csv')\n",
    "complex_question1 = pd.read_csv('score_check/question/complex_questions_1.csv')\n",
    "complex_question2 = pd.read_csv('score_check/question/complex_questions_2.csv')\n",
    "\n",
    "final_question = pd.concat([simple_question1, simple_question2, complex_question1, complex_question2], ignore_index=True)\n",
    "final_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = []\n",
    "\n",
    "for i in final_question['질문']:\n",
    "    response, retrieved_docs, ranked_docs = run_pipeline(i)\n",
    "\n",
    "    final_answer.append({\n",
    "        \"질문\": i,\n",
    "        \"내 모델 답변\": response,\n",
    "        \"검색된 문서\": retrieved_docs,\n",
    "        \"재정렬된 문서\": ranked_docs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pd = pd.DataFrame(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
